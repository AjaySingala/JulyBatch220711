Apache Spark:
=============
Framework to process large amounts of data.
Clustered computing.
Runs on memory - is very fast.
Faster than MapReduce.
	- MR uses a lot of disk i/o for processing.
	- Spark does processing in-memory, hence is almost 100 times faster than MR.

Considered to be a "Hadoop Killer" - MapReduce of Hadoop killer.
Hadoop originally was not built for cloud. It was built many years before Cloud became popular for huge workloads.
Spark was designed and created keeping in mind huge workloads on the cloud!
	- It also supports cloud storage like S3.
	- Easily integtrates with Hadoop environment like HDFS, YARN, HBase, Hive.
Spark replaces MapReduce for batch processing.
Spark features:
	- Speed:
		-Spark is faster than Hadoop because it uses "in-memory cluster computing".
			- processing is done parallelly on multiple nodes.
	- Supports multiple languages: built-in APIs in Java, Scala, Python.
	- Advanced analytics: Supports not only MapReduce, but also SQL queries, Streaming ML, Graph algorithms etc.
	- Spark is written in Scala.

Components of Spark:
	- Apache Spark Core
	- SparkSQL
	- Spark Streaming
	- MLib (Machine Learning)
	- GraphX
	
Spark Architecture:
-------------------
(refer to diagram)

How many executors required?
How to calculate executor memory?
Consider a cluster with:
	- Nodes = 10
	- Each node Cores = 16 (-1 of OS)
	- Each node RAM = 62 GB (-1 for OS)

No. of cores is the no. of concurrent tasks an executor can run in parallel. So, the general rule of thumb for optimal value is 5.

To calculate the no, of executors:
No. of executors 	= No. of cores / concurrent tasks
					= 15 / 5
					= 3
					
no. of total executos for the Spark job = No. of nodes * no. of executors in each node
										= 10 * 3
										= 30 executors per spark job.

Define executor memory:
	spark.executor.memory 

RDDs:
=====
Resilient Distributed Datasets
	- Resilient: able to withstand or recover quickly from failure or difficult conditions.
	- Distributed: distributed across nodes on the clustet.
	- Datasets: data that is represneted in rows and columns.
	
Require 2 things:
1. SparkSession
2. SparkContext

When we us the spark-shell, we get these by default.
- Spark context available as 'sc'.
- Spark session available as 'spark'.

To read a text file into an RDD (of strings):
val file = sc.textFile("file.txt")

Once you have the RDD, you perform Transformations and Actions.

Try a wordcount:
val counts = file.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)
Gives an ERROR. Looks for the file on HDFS.

Specify local path:
val file = sc.textFile("file:///home/maria_dev/SparkSamples/sparkscala/src/main/scala/example/twinkle.txt")

Try again:
val counts = file.flatMap(line => line.split(" ")).map(word => (word,1)).reduceByKey(_+_)

Display results:
counts.collect()						// Action.
counts.saveAsTextFile("path")			// Action, that saves the RDD on HDFS into files inside a folder named "path".
counts.saveAsTextFile("file://path")	// Action, that saves the RDD on local machine into files inside a folder named "path".
counts.cache()							// Transformation. Persist in cache.
counts.unpersist()						// Unpersists the storage.

SparkContext and SparkSession:
==============================
SparkContext (SC):
-------------
In previous version of Spark (includes PySpark), SC was the entry point for Spark programming with RDD and to connect to the cluster.
With Spark 2.0, SparkSession (SS) was introduced and became the entry point for programming with Datasets (DS) and DataFrames (DF).

SC is part of org.apache.spark package, used to create RDDs, accumulators, broadcast variables on the cluster. The object in spark-shell is "sc".
Since Spark 2.0, most of the functionalities available in SC are also available in SS.
SS is the new entry point, used to create RDDs, DF and DS. The object in spark-shell is "spark".

How to load your code from your dev machine to VM so that you can run the spark jo on your vm?
1) On the dev machine, create a package in sbt by running "package" in sbt.
2) copy the .jar file from dev PC to VM using the scp command:
scp -P 2222 *.jar maria_dev@sandbox-hdp.hortonworks.com:/home/maria_dev
3) on the VM, run spark-submit:
spark-submit ./sparkscala_2.11-0.1.0-SNAPSHOT.jar --class sparkbasics.WordCountExample

RDD Transformations:
--------------------
map(func)
filter(func)
flatMap(func)
union(anotherRDD)
intersection(anotherRDD)
distinct()
groupByKey()	: Works on a set of K-V pairs, returns a dataset of (K, Iterable<V>) pairs.
reduceByKey()
sortByKey()		: requires a set of K-V pairs
	- sortByKey(ascending=false)
join(otherRDD)
coalesce(numOfPartitions)	: Reduce the no. of partitions.
repartition(numOfPartitions)

RDD Actions:
------------
collect()
count()
first()	: similar to take(1).
take(n)	: an array of the first "n" elements.
saveAsTextFile(path)
foreach(func)
countByKey()	: works only on RDDs that have K-V pairs.

Transformation vs Action:
T: is lazy in operation. Gets executed only when you perform an action on the RDD.

RDD Lineage:
------------
Transformations allow you to create dependencies between RDDs.
Each RDD in the dependency chain has a function for calculating its data and has a pointer (dependency) to its parent RDD.

03-Aug-2022:
============
Creating an RDD using the sc.parallellize() method:
val numbers = sc.parallelize(Seq(("A",1), ("B", 2), ("C",3)))

sc.parallelize(): It distributes the data in the RDD across nodes on the cluster.

RDD join() and groupBy():
-------------------------
groupBy() vs cogroup():
	For groupBy(), you work on a single dataset (RDD, DF).
		val newrdd = rdd1.groupBy(func)
	For cogroup(), you work on 2 or more datasets.
		val newrdd = rdd1.cogroup(rdd2)
		
Shared Variables:
=================
- Broadcast variables
- Accumulators

Broadcast variables:
--------------------
eCommerce data set that has order data that has details about orders placed. For e.g.;
OrderId,OrderDate,CustomerId,ProductId,Qty,Rate

ProductName from Products data set.
CustomerName, City, State from Customers data set.

Probably, join these data sets together to perform some aggregations etc.
Since they are RDDs, all 3 data sets will be distributed across nodes for the processing.

Instead use Broadcast Variables (BV).
BVs read-only variables cached on each machine rather than shipping a copy with the tasks.
For e.g.; when you create BV RDD of the Products data set, it will make this BV available to the nodes in a very efficient, reduced communication algorithm for the tasks to access.
When you do a join (or use the BV along with another data set), the BV is not sent to the nodes with the task. It is used by the nodes from the cache that was made available as part of the BV config.

Syntax:
spark.sparkContext.broadcast(data)

Accumulators:
-------------
When you want to "add" values ("sum" operation) on a set of data that is spread across the cluster.

Format of log file:
city,locality,category of item sold,value of the item sold.
Dallas,Central Ave,Groceries,38

Reston,Main St.,Hair Care, 23
Bad data packet
Boston,Qunicy Ave,Beverages,65

var blankLines: Int = 0
sc.testFile("myapp.log").foreach{ line => if(line.length == 0) blankLines += 1 } 
println(s"There are $blankLines in the file.")

The problem with this code is that when the driver program runs and prints the value of the variable blankLines, it may not give the correct results because Spark ships the code to every executor on the cluster, so the variable becomes local to that executor, and hence it is updated locally and not passed back to the driver. Due to this, the driver program will print it a zero.

To overcome this, we need to use some kind of shared variable that can be accumulated across executors. And that shared variable is Accumulators in Spark.

So the code will look something like this:
var blankLines = sc.accumulator(0, "Blank Lines")
sc.testFile("myapp.log").foreach{ line => if(line.length == 0) blankLines += 1 } 
println(s"There are $blankLines in the file.")

Paired RDDs:
============
Key-Value Pair RDDs.
Transformations: groupByKey, reduceByKey, sortByKey
Actions: countByKey

collectAsMap():
(Brazil,1)
(Canada,1)
(Germany,1)
(China,1)
(Russia,1)
(India,1)
(USA,1)

collect():
(Germany,1)
(India,1)
(USA,1)
(USA,1)
(India,1)
(Russia,1)
(India,1)
(Brazil,1)
(Canada,1)
(China,1)

aggregateByKey on Paired RDDs:
------------------------------
It is another way of using reduceByKey(), but a bit more complicated.

Because aggregateByKey has 3 main inputs/functions, 1 of them works on output of each of the RDD partition transformation. And if there is only single partition in RDD then that function will not be called.

aggregateByKey requires 3 main inputs:
1.	zeroValue: Initial value (mostly Zero (0)) which will not affect the aggregate values to be collected. For example, 0 would be initial value to perform sum or count or to perform operation on String then the initial value will be empty string.
2.	Combiner function: This function accepts two parameters. The second parameter is merged into the first parameter. This function combines/merges values within a single partition.
3.	Reduce/Merge function: This function also accepts two parameters. Here, parameters are merged into one across RDD partitions.
Syntax:
dataframeRDD.aggregateByKey(init_value)(combinerFunc,reduceFunc)

For e.g.;
    def param1= (accu:Int,v:Int) => accu + v
    def param2= (accu1:Int,accu2:Int) => accu1 + accu2
    println("Aggregate by Key ==> wordcount")
    val wordCount2 = pairRDD.aggregateByKey(0)(param1,param2)
	
04-Aug-2022:
============
SparkContext and SparkSession:
------------------------------
What is SparkContext (SC)?
The driver program (Spark application / job) uses the SparkContext to connect and communicate with the cluster and it helps in executing and coordinating the Spark job with the resource managers (YARN, Mesos).
Using SC, you can get access to other contexts like SQLContext, HiveContext etc.
Use it to set configuration parameters for the spark jobs.

What is SparkSession (SS)?
It was introduced in Spark 2.0.
Once you have a SS, you automatically have access to SC as well.
SS is the new entry of Spark and it replaces the old SQLContext and HiveContext, although they are still available for backward compatibility.
Once we have access to a SS, we can start working with DataFrames / DataSets.

SparkSQL & DataFrames:
======================
DataSets (DS):
--------------
DS is a distributed collection of data.
Very similar to a RDD, but strongly typed and has benefits of SparkSQL's optimized execution engine.
DS API is available in Scala and Java.
It is not available in Python. You cannot use SparkSQL DS in Python.

DataFrame (DF):
---------------
A DF is a DS organized in named columns.
It is equivalent to a table (in a RDBMS) with rows and columns.
But has better optmization under the hood.
Create DFs from various sources: files, DBs, Hive, RDDs.
DF API is available in Scala, Java, Python and R.
In Scala and Java, a DF is represented by a DataSet of Rows, i.e.; Java: DataSet<Row>. Scala: DataSet[Row].

Features of DS / DF:
- Optimized query feature: Catalyst Query Optimizer framework.
- Compile-time analysis: checks for syntaxes at compile time. 
- Interconvertible: you can convert RDDs DFs, DFs to DSs, DSs to DFs.
- Faster computation: DS are faster than RDDs. DFs are faster than DS.
- Can also use persistent storage, if required.
- Less memory consumed : makes use of caching to create optimal data layouts.
- Single API that can be used from Scala and Java.

Executing SQL queries on DFs:
-----------------------------
1. Create a temporary view from the DF.
	df.createOrReplaceTempView("viewname")
	This temp view is available as long as your SparkSession is active.
	Temp view are only available within the SparkSession where they have been created.
2. Run your sql query on the temp view (returns a new DF):
	val newdf = spark.sql("query")
	val newdf = spark.sql("SELECT * FROM people_view")
	
Global Temporary View (GTV):
	- There is a Global Temporary View as well.
	- Syntax: df.createGlobalTempView("people")
	- GTV are available across SparkSessions.
	- To use the GTV, use the db name global_temp as follows:
	spark.sql("SELECT * FROM global_temp.people").show()
	
